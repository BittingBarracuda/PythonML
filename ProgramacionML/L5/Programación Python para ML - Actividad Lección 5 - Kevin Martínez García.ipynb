{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67dff642",
   "metadata": {},
   "source": [
    "# Carga del dataset *Climate Model Simulation Crashes*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5580b04",
   "metadata": {},
   "source": [
    "Para comenzar con esta actividad, realizamos la carga del *Climate Model Simulation Crashes Data Set* disponible en la plataforma *UCI Machine Learning Repository* (https://archive.ics.uci.edu/ml/datasets/climate+model+simulation+crashes)\n",
    "\n",
    "Comenzamos realizando la carga mediante ``pandas``. En este caso, el archivo proporcionado contiene una cabecera con los identificadores de cada columna. Sin embargo, el fichero presentaba una dificultad adicional y es que cada columna estaba separada por un número aparentemente arbitrario de espacios, lo que complicaba su lectura. Para solventar este error, se abrió dicho fichero desde *VS Code* y, mediante el uso de la expresión regular \" +\", se reemplazaron todos los espacios por una coma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd371537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Study</th>\n",
       "      <th>Run</th>\n",
       "      <th>vconst_corr</th>\n",
       "      <th>vconst_2</th>\n",
       "      <th>vconst_3</th>\n",
       "      <th>vconst_4</th>\n",
       "      <th>vconst_5</th>\n",
       "      <th>vconst_7</th>\n",
       "      <th>ah_corr</th>\n",
       "      <th>ah_bolus</th>\n",
       "      <th>...</th>\n",
       "      <th>efficiency_factor</th>\n",
       "      <th>tidal_mix_max</th>\n",
       "      <th>vertical_decay_scale</th>\n",
       "      <th>convect_corr</th>\n",
       "      <th>bckgrnd_vdc1</th>\n",
       "      <th>bckgrnd_vdc_ban</th>\n",
       "      <th>bckgrnd_vdc_eq</th>\n",
       "      <th>bckgrnd_vdc_psim</th>\n",
       "      <th>Prandtl</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.859036</td>\n",
       "      <td>0.927825</td>\n",
       "      <td>0.252866</td>\n",
       "      <td>0.298838</td>\n",
       "      <td>0.170521</td>\n",
       "      <td>0.735936</td>\n",
       "      <td>0.428325</td>\n",
       "      <td>0.567947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245675</td>\n",
       "      <td>0.104226</td>\n",
       "      <td>0.869091</td>\n",
       "      <td>0.997518</td>\n",
       "      <td>0.448620</td>\n",
       "      <td>0.307522</td>\n",
       "      <td>0.858310</td>\n",
       "      <td>0.796997</td>\n",
       "      <td>0.869893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.606041</td>\n",
       "      <td>0.457728</td>\n",
       "      <td>0.359448</td>\n",
       "      <td>0.306957</td>\n",
       "      <td>0.843331</td>\n",
       "      <td>0.934851</td>\n",
       "      <td>0.444572</td>\n",
       "      <td>0.828015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616870</td>\n",
       "      <td>0.975786</td>\n",
       "      <td>0.914344</td>\n",
       "      <td>0.845247</td>\n",
       "      <td>0.864152</td>\n",
       "      <td>0.346713</td>\n",
       "      <td>0.356573</td>\n",
       "      <td>0.438447</td>\n",
       "      <td>0.512256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.997600</td>\n",
       "      <td>0.373238</td>\n",
       "      <td>0.517399</td>\n",
       "      <td>0.504993</td>\n",
       "      <td>0.618903</td>\n",
       "      <td>0.605571</td>\n",
       "      <td>0.746225</td>\n",
       "      <td>0.195928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.679355</td>\n",
       "      <td>0.803413</td>\n",
       "      <td>0.643995</td>\n",
       "      <td>0.718441</td>\n",
       "      <td>0.924775</td>\n",
       "      <td>0.315371</td>\n",
       "      <td>0.250642</td>\n",
       "      <td>0.285636</td>\n",
       "      <td>0.365858</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.783408</td>\n",
       "      <td>0.104055</td>\n",
       "      <td>0.197533</td>\n",
       "      <td>0.421837</td>\n",
       "      <td>0.742056</td>\n",
       "      <td>0.490828</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>0.392123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471463</td>\n",
       "      <td>0.597879</td>\n",
       "      <td>0.761659</td>\n",
       "      <td>0.362751</td>\n",
       "      <td>0.912819</td>\n",
       "      <td>0.977971</td>\n",
       "      <td>0.845921</td>\n",
       "      <td>0.699431</td>\n",
       "      <td>0.475987</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.513199</td>\n",
       "      <td>0.061812</td>\n",
       "      <td>0.635837</td>\n",
       "      <td>0.844798</td>\n",
       "      <td>0.441502</td>\n",
       "      <td>0.191926</td>\n",
       "      <td>0.487546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551543</td>\n",
       "      <td>0.743877</td>\n",
       "      <td>0.312349</td>\n",
       "      <td>0.650223</td>\n",
       "      <td>0.522261</td>\n",
       "      <td>0.043545</td>\n",
       "      <td>0.376660</td>\n",
       "      <td>0.280098</td>\n",
       "      <td>0.132283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Study  Run  vconst_corr  vconst_2  vconst_3  vconst_4  vconst_5  vconst_7  \\\n",
       "0      1    1     0.859036  0.927825  0.252866  0.298838  0.170521  0.735936   \n",
       "1      1    2     0.606041  0.457728  0.359448  0.306957  0.843331  0.934851   \n",
       "2      1    3     0.997600  0.373238  0.517399  0.504993  0.618903  0.605571   \n",
       "3      1    4     0.783408  0.104055  0.197533  0.421837  0.742056  0.490828   \n",
       "4      1    5     0.406250  0.513199  0.061812  0.635837  0.844798  0.441502   \n",
       "\n",
       "    ah_corr  ah_bolus  ...  efficiency_factor  tidal_mix_max  \\\n",
       "0  0.428325  0.567947  ...           0.245675       0.104226   \n",
       "1  0.444572  0.828015  ...           0.616870       0.975786   \n",
       "2  0.746225  0.195928  ...           0.679355       0.803413   \n",
       "3  0.005525  0.392123  ...           0.471463       0.597879   \n",
       "4  0.191926  0.487546  ...           0.551543       0.743877   \n",
       "\n",
       "   vertical_decay_scale  convect_corr  bckgrnd_vdc1  bckgrnd_vdc_ban  \\\n",
       "0              0.869091      0.997518      0.448620         0.307522   \n",
       "1              0.914344      0.845247      0.864152         0.346713   \n",
       "2              0.643995      0.718441      0.924775         0.315371   \n",
       "3              0.761659      0.362751      0.912819         0.977971   \n",
       "4              0.312349      0.650223      0.522261         0.043545   \n",
       "\n",
       "   bckgrnd_vdc_eq  bckgrnd_vdc_psim   Prandtl  outcome  \n",
       "0        0.858310          0.796997  0.869893        0  \n",
       "1        0.356573          0.438447  0.512256        1  \n",
       "2        0.250642          0.285636  0.365858        1  \n",
       "3        0.845921          0.699431  0.475987        1  \n",
       "4        0.376660          0.280098  0.132283        1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = 'pop_failures.dat'\n",
    "df = pd.read_csv(filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8ad50",
   "metadata": {},
   "source": [
    "# Conteo de clases y desequilibrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a918622",
   "metadata": {},
   "source": [
    "Procedemos a hacer un conteo de las diferentes clases en el dataset haciendo uso del método ``.value_counts()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e47a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    494\n",
       "0     46\n",
       "Name: outcome, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae65ba",
   "metadata": {},
   "source": [
    "Como podemos observar, el dataset cuenta con un desequilibrado notable de los datos. En concreto, contamos con 540 datos, el ~91.48% etiquetados como \"1\" y el resto como \"0\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34da0bb",
   "metadata": {},
   "source": [
    "# Técnicas de validación cruzada con KNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f9fcc",
   "metadata": {},
   "source": [
    "Vamos a proceder a analizar el rendimiento del modelo *k-Nearest Neighbors* utilizando los siguientes métodos de validación cruzada:\n",
    "\n",
    "1.- Validación con *K-folds*\n",
    "\n",
    "2.- Validación con *Leave-One-Out*\n",
    "\n",
    "3.- Validación con *Monte Carlo*\n",
    "\n",
    "Al tratarse de un problema de clasificación binaria, utilizaremos métricas de clasificación para determinar el rendimiento obtenido. En concreto, al tratarse de un problema desequilibrado descartamos el uso de la *accuracy* como métrica del rendimento. Como vimos en sesiones de teoría, el *accuracy* puede llevar a confusión cuando nuestro dataset tiene un número no equitativo de muestras en cada clase. Por ejemplo, supongamos un dataset con 1000 instancias, 990 de las cuales forman parte de la clase \"1\" y 10 de la clase \"0\" y un modelo que, ante cualquier entrada, devuelve \"1\" como la clase predicha. Este modelo arrojaría un valor del 99% de *accuracy* a pesar de que clasifica de forma incorrecta todas las instancias de clase \"0\". En general, la tendencia de modelos entrenados con datasets desequilibrados es que se favorezca la clasificación en las clases con un número mayoritario de instancias. \n",
    "\n",
    "Una mejor métrica para nuestro dataset podría ser la *balanced accuracy* que se obtiene como la media aritmética entre *sensibilidad* y la *especificidad*. Recordemos que la *sensibilidad* (o *true positive rate*) se define como el porcentaje de patrones positivos predichos como positivos, mientras que la *especificidad* se define como el porcentaje de patrones negativos predichos como negativos. Su definición (en base a la matriz de confusión) es la que sigue:\n",
    "\n",
    "$Sensiblidiad = \\frac{TP}{TP + FN}$\n",
    "\n",
    "$Especificidad = \\frac{TN}{TN + FP}$\n",
    "\n",
    "Si identificamos \"1\" como clase positiva y \"0\" como clase negativa tendríamos lo siguiente:\n",
    "\n",
    "- Por una parte, nuestro modelo tendría una **sensibilidad alta** puesto que consigue muchos *true positives* y pocos *false negatives*. En el caso del ejemplo anterior, al tener 0 *false negatives*, tendríamos una sensibiliad de 1.\n",
    "- Por otra parte, nuestro modelo tendría una **especificidad baja** puesto que el número de *true negatives* es bajo. En el caso del ejemplo anterior, al tener 0 true negatives, tendríamos una especificidad de 0.\n",
    "\n",
    "Si calculamos *balanced accuracy* como:\n",
    "\n",
    "$Balanced\\_Acc = \\frac{Sensiblidad + Especificidad}{2}$\n",
    "\n",
    "Podemos ver como esta magnitud \"penaliza\" situaciones como la anterior, consiguiendo una *balanced accuracy* más próxima a 0.5 que a 1. En el caso propuesto más arriba, obtendríamos una *balanced accuracy* de 0.5 mostrando así la tendencia a clasificar todas las muestras como \"1\". Si bien esta métrica resulta adecuada para el problema que tratamos, se plantea una problemática relacionada con el uso de la función ``cross_val_score()``. Como se ha estudiado, esta función calcula la métrica especificada con la *keyword* ``scoring`` para cada una de las particiones del dataset. En caso de la validación *Leave-One-Out* únicamente se calcula una predicción, por lo que nuestra matriz de confusión únicamente tendrá un valor no nulo y eso podría generar operaciones no deseades al calcular métricas como la *sensibilidad* o la *especificidad*. Es más, la propia función ``cross_val_score()`` arroja un *warning* al tratar de calcularlas con *Leave-One-Out*.  A continuación, propongo un cálculo alternativo de la *matriz de confusión* consistente en almacenar las predicciones sobre todas las instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7ff91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[492.,  46.],\n",
       "       [  2.,   0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, LeaveOneOut, ShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Separamos variables de entrada X y variable objetivo Y\n",
    "X = df[df.columns[:-1]]\n",
    "Y = df['outcome']\n",
    "\n",
    "# Incializamos una semilla para poder replicar el reparto\n",
    "seed = random.randint(0, 10)\n",
    "\n",
    "# Obtenemos los objetos de validación cruzada\n",
    "kfold = KFold(n_splits = 10, random_state = seed, shuffle = True)\n",
    "loocv = LeaveOneOut()\n",
    "monte_carlo = ShuffleSplit(n_splits = 100, test_size = 0.3, random_state = seed)\n",
    "\n",
    "# Generamos el modelo de KNeighbors\n",
    "model_knn = KNeighborsClassifier()\n",
    "\n",
    "# Entrenamos el modelo con los diferentes métodos de validación cruzada\n",
    "res_kfold = cross_val_score(model_knn, X, Y, cv = kfold, scoring = 'balanced_accuracy')\n",
    "# res_loocv = cross_val_score(model_knn, X, Y, cv = loocv, scoring = 'balanced_accuracy')\n",
    "res_monte_carlo = cross_val_score(model_knn, X, Y, cv = monte_carlo, scoring = 'balanced_accuracy')\n",
    "\n",
    "# Pasamos a obtener las métricas de LeaveOneOut()\n",
    "y_true, y_pred = list(), list()\n",
    "# Almacenamos el conjunto de entrada y sus etiquetas como arrays numpy\n",
    "X_aux, y_aux = X.values, Y.values\n",
    "# Obtenemos los diferentes splits que genera Leave-One-Out\n",
    "for train, test in loocv.split(X_aux):\n",
    "    # Calculamos el split (n-1 datos de entrenamiento, 1 de test)\n",
    "    X_tr, X_te = X_aux[train, :], X_aux[test, :]\n",
    "    y_tr, y_te = y_aux[train], y_aux[test]\n",
    "    # Ajustamos el modelo\n",
    "    model_knn.fit(X_tr, y_tr)\n",
    "    # Calculamos la predicción con el elemento de test\n",
    "    y_new = model_knn.predict(X_te)\n",
    "    # Nos quedamos con el valor verdadero y con el valor predicho\n",
    "    y_true.append(y_te[0])\n",
    "    y_pred.append(y_new[0])\n",
    "    \n",
    "# Calculamos la matriz de confusión para un problema de clas. binaria\n",
    "conf_mat = np.zeros(shape = (2, 2))\n",
    "conf_mat[0, 0] = len(list(filter(lambda x: x[0] == x[1] == 1, zip(y_true, y_pred)))) # TP\n",
    "conf_mat[0, 1] = len(list(filter(lambda x: x[0] != x[1] and x[1] == 1, zip(y_true, y_pred)))) # FP\n",
    "conf_mat[1, 0] = len(list(filter(lambda x: x[0] != x[1] and x[1] == 0, zip(y_true, y_pred)))) # FN \n",
    "conf_mat[1, 1] = len(list(filter(lambda x: x[0] == x[1] == 0, zip(y_true, y_pred)))) # TN\n",
    "# Mostramos la matriz obtenida\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8920dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos sensibilidad y especificidad\n",
    "tp, fp, fn, tn = conf_mat[0, 0], conf_mat[0, 1], conf_mat[1, 0], conf_mat[1, 1]\n",
    "sens = tp / (tp + fn)\n",
    "spec = tn / (tn + fp)\n",
    "res_loocv = (sens + spec) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "346b4b14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas KFOLD -> Balanced Accuracy Promedio: 0.49803846153846154 \t Desv. estándar: 0.003924019494610455\n",
      "Métricas LOOCV -> Balanced Accuracy: 0.4979757085020243\n",
      "Métricas MonteCarlo -> Balanced Accuracy Promedio: 0.49906148804991785 \t Desv. estándar: 0.002063295596979471\n"
     ]
    }
   ],
   "source": [
    "print(f\"Métricas KFOLD -> Balanced Accuracy Promedio: {res_kfold.mean()} \\t Desv. estándar: {res_kfold.std()}\")\n",
    "print(f\"Métricas LOOCV -> Balanced Accuracy: {res_loocv}\") #\\t Desv. estándar: {res_loocv.std()}\")\n",
    "print(f\"Métricas MonteCarlo -> Balanced Accuracy Promedio: {res_monte_carlo.mean()} \\t Desv. estándar: {res_monte_carlo.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e99e30",
   "metadata": {},
   "source": [
    "Como puede observarse de las métricas anteriores, nuestro clasificador parece seguir la tendencia del ejemplo de más arriba independientemente del método de validación cruzada utilizado. Vemos que la *balanced accuracy* toma un valor cercano a 0.5 en todos los casos, lo que indicaría una *sensibilidad* alta y una *especificidad* baja. Consideramos que esta métrica es más informativa que otras en este caso particular pues, si el modelo se utilizase para hacer predicciones sobre un conjunto de datos con un número elevado de instancias de clase \"0\", se obtendrían unos resultados poco satisfactorios.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d757550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'matthews_corrcoef',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_brier_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_absolute_percentage_error',\n",
       " 'neg_mean_gamma_deviance',\n",
       " 'neg_mean_poisson_deviance',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'neg_root_mean_squared_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'rand_score',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'roc_auc_ovo',\n",
       " 'roc_auc_ovo_weighted',\n",
       " 'roc_auc_ovr',\n",
       " 'roc_auc_ovr_weighted',\n",
       " 'top_k_accuracy',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.get_scorer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177f8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "12749f567798517b8543354a13719bbd42e9e3e56a89ba27a040f4f72d5c2230"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprobación de la instalación de ``TensorFlow`` y ``Keras``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comenzar, realizamos el importado de estas dos librerías y mostramos su versión para comprobar la correcta instalación de las mismas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga y reestructurado del *MNIST Data Set*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos realizando la carga del *MNIST Data Set* disponible desde la propia librería ``Keras``. Mostramos por pantalla las dimensiones de los conjuntos cargados para comprobar que tratamos con matrices en tres dimensiones. Esto es así porque cada imágen del conjunto de datos cuenta con 28 píxeles de altura y 28 de ancho. Realizaremos un proceso de \"aplanado\" (``flatten``) para representar este conjunto de datos en forma de tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos las clases del dataset (10 clases, dígitos entre 0 y 9) y el número de instancias por clase. En este caso, todas las clases tienen alrededor de 6000 instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases en el data set -> [0 1 2 3 4 5 6 7 8 9]\n",
      "Número de instancias por clase -> [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_classes, n_instances = np.unique(y_train, return_counts = True)\n",
    "print(f'Clases en el data set -> {n_classes}')\n",
    "print(f'Número de instancias por clase -> {n_instances}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se ha comentado previamente, realizamos un \"aplanado\" de las matrices, en este caso, haremos uso de la función ``reshape`` de ``numpy`` con el segundo parámetro fijado a ``-1`` para que la propia librería infiera las nuevas dimensiones de la matriz. En este caso, obtenemos instancias con 784 características (28*28 = 784 píxeles por imágen, cada píxel es una tonalidad de gris que representa una característica de la imágen.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de proceder con el ajuste de nuestro modelo, debemos recordar que las redes neuronales se basan en el cácluclo de distancias, por lo que se beneficia de un proceso de escalado de sus variables. Para ello utilizaremos ``StandardScaler`` de la librería ``sklearn``. \n",
    "\n",
    "Por otra parte resulta conveniente discutir la estructura de la red neuronal que vamos a diseñar. La estructura de nuestra red será la siguiente.\n",
    "\n",
    "- Una capa de entrada con 784 nodos, uno por cada característica de nuestras instancias.\n",
    "- Tres capas ocultas con 128 nodos y función de activación. Entre cada una de estas capas ocultas incluimos una capa ``Dropout`` que de forma aleatoria pone a 0 un 10% de las conexiones, evitando de esta forma el *overfitting*.\n",
    "- Una capa de salida con tantos nodos como clases, en este caso, 10 nodos.\n",
    "\n",
    "Durante el entrenamiento, realizaremos una optimización mediante el algoritmo de Adam, y utilizaremos la ``categorial_crossentropy`` como función de pérdida. Esta magnitud calcula la diferencia media entre la distribución de probabilidad predicha frente a la distribución de probabilidad real de cada clase del problema. Buscamos minimizar esta función de pérdida. \n",
    "\n",
    "En este caso, como entendemos que los nodos de salida representan probabilidad de pertenencia a cada clase, necesitamos codificar las variables objetivo como *one-hot*. Para ello, utilizaremos la función ``to_categorical()`` de ``keras.utils``. Además, la capa de salida de nuestra red tendrá una función de activación ``softmax`` para, precisamente, predecir la probabilidad de cada clase.\n",
    "\n",
    "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2740 - accuracy: 0.9198 - val_loss: 0.1432 - val_accuracy: 0.9590\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1463 - accuracy: 0.9567 - val_loss: 0.1102 - val_accuracy: 0.9671\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1106 - accuracy: 0.9664 - val_loss: 0.1128 - val_accuracy: 0.9701\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0983 - accuracy: 0.9698 - val_loss: 0.1151 - val_accuracy: 0.9684\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0855 - accuracy: 0.9739 - val_loss: 0.0994 - val_accuracy: 0.9723\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0731 - accuracy: 0.9776 - val_loss: 0.1020 - val_accuracy: 0.9736\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0684 - accuracy: 0.9788 - val_loss: 0.1057 - val_accuracy: 0.9742\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0627 - accuracy: 0.9801 - val_loss: 0.1162 - val_accuracy: 0.9753\n",
      "Epoch 9/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0620 - accuracy: 0.9809 - val_loss: 0.1056 - val_accuracy: 0.9755\n",
      "Epoch 10/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0577 - accuracy: 0.9828 - val_loss: 0.1030 - val_accuracy: 0.9759\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0493 - accuracy: 0.9854 - val_loss: 0.1220 - val_accuracy: 0.9741\n",
      "Epoch 12/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0529 - accuracy: 0.9840 - val_loss: 0.1136 - val_accuracy: 0.9767\n",
      "Epoch 13/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0453 - accuracy: 0.9862 - val_loss: 0.1339 - val_accuracy: 0.9745\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0493 - accuracy: 0.9858 - val_loss: 0.1274 - val_accuracy: 0.9744\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0451 - accuracy: 0.9868 - val_loss: 0.1419 - val_accuracy: 0.9728\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0439 - accuracy: 0.9870 - val_loss: 0.1250 - val_accuracy: 0.9764\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0409 - accuracy: 0.9879 - val_loss: 0.1547 - val_accuracy: 0.9754\n",
      "Epoch 18/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0437 - accuracy: 0.9870 - val_loss: 0.1474 - val_accuracy: 0.9760\n",
      "Epoch 19/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0404 - accuracy: 0.9885 - val_loss: 0.1263 - val_accuracy: 0.9768\n",
      "Epoch 20/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0399 - accuracy: 0.9887 - val_loss: 0.1505 - val_accuracy: 0.9737\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0418 - accuracy: 0.9885 - val_loss: 0.1635 - val_accuracy: 0.9758\n",
      "Epoch 22/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0436 - accuracy: 0.9886 - val_loss: 0.1423 - val_accuracy: 0.9775\n",
      "Epoch 23/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0399 - accuracy: 0.9900 - val_loss: 0.1416 - val_accuracy: 0.9765\n",
      "Epoch 24/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0364 - accuracy: 0.9902 - val_loss: 0.1226 - val_accuracy: 0.9758\n",
      "Epoch 25/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0400 - accuracy: 0.9899 - val_loss: 0.1930 - val_accuracy: 0.9740\n",
      "Epoch 26/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0388 - accuracy: 0.9903 - val_loss: 0.1484 - val_accuracy: 0.9764\n",
      "Epoch 27/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0338 - accuracy: 0.9910 - val_loss: 0.1394 - val_accuracy: 0.9760\n",
      "Epoch 28/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0365 - accuracy: 0.9905 - val_loss: 0.1897 - val_accuracy: 0.9759\n",
      "Epoch 29/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0337 - accuracy: 0.9903 - val_loss: 0.1625 - val_accuracy: 0.9744\n",
      "Epoch 30/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0362 - accuracy: 0.9904 - val_loss: 0.1429 - val_accuracy: 0.9775\n",
      "Epoch 31/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0317 - accuracy: 0.9912 - val_loss: 0.1570 - val_accuracy: 0.9774\n",
      "Epoch 32/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0364 - accuracy: 0.9909 - val_loss: 0.1602 - val_accuracy: 0.9762\n",
      "Epoch 33/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0336 - accuracy: 0.9911 - val_loss: 0.1525 - val_accuracy: 0.9776\n",
      "Epoch 34/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0348 - accuracy: 0.9912 - val_loss: 0.1501 - val_accuracy: 0.9774\n",
      "Epoch 35/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0328 - accuracy: 0.9919 - val_loss: 0.1498 - val_accuracy: 0.9779\n",
      "Epoch 36/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0339 - accuracy: 0.9911 - val_loss: 0.1636 - val_accuracy: 0.9769\n",
      "Epoch 37/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0314 - accuracy: 0.9924 - val_loss: 0.1372 - val_accuracy: 0.9768\n",
      "Epoch 38/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0341 - accuracy: 0.9917 - val_loss: 0.1671 - val_accuracy: 0.9780\n",
      "Epoch 39/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0310 - accuracy: 0.9923 - val_loss: 0.1755 - val_accuracy: 0.9761\n",
      "Epoch 40/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0356 - accuracy: 0.9914 - val_loss: 0.1462 - val_accuracy: 0.9780\n",
      "Epoch 41/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0342 - accuracy: 0.9913 - val_loss: 0.1700 - val_accuracy: 0.9767\n",
      "Epoch 42/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0356 - accuracy: 0.9917 - val_loss: 0.1690 - val_accuracy: 0.9780\n",
      "Epoch 43/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0284 - accuracy: 0.9931 - val_loss: 0.2136 - val_accuracy: 0.9772\n",
      "Epoch 44/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0278 - accuracy: 0.9924 - val_loss: 0.1847 - val_accuracy: 0.9777\n",
      "Epoch 45/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0320 - accuracy: 0.9922 - val_loss: 0.1628 - val_accuracy: 0.9772\n",
      "Epoch 46/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0337 - accuracy: 0.9923 - val_loss: 0.1771 - val_accuracy: 0.9767\n",
      "Epoch 47/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0325 - accuracy: 0.9918 - val_loss: 0.2108 - val_accuracy: 0.9759\n",
      "Epoch 48/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0346 - accuracy: 0.9926 - val_loss: 0.1882 - val_accuracy: 0.9787\n",
      "Epoch 49/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0293 - accuracy: 0.9928 - val_loss: 0.1768 - val_accuracy: 0.9757\n",
      "Epoch 50/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0283 - accuracy: 0.9930 - val_loss: 0.2168 - val_accuracy: 0.9739\n",
      "Epoch 51/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0273 - accuracy: 0.9928 - val_loss: 0.2283 - val_accuracy: 0.9775\n",
      "Epoch 52/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0325 - accuracy: 0.9928 - val_loss: 0.2150 - val_accuracy: 0.9781\n",
      "Epoch 53/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0294 - accuracy: 0.9931 - val_loss: 0.2093 - val_accuracy: 0.9754\n",
      "Epoch 54/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0328 - accuracy: 0.9924 - val_loss: 0.1585 - val_accuracy: 0.9779\n",
      "Epoch 55/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0300 - accuracy: 0.9931 - val_loss: 0.2140 - val_accuracy: 0.9766\n",
      "Epoch 56/100\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0248 - accuracy: 0.9937 - val_loss: 0.2269 - val_accuracy: 0.9770\n",
      "Epoch 57/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0300 - accuracy: 0.9934 - val_loss: 0.1934 - val_accuracy: 0.9772\n",
      "Epoch 58/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0328 - accuracy: 0.9931 - val_loss: 0.2254 - val_accuracy: 0.9772\n",
      "Epoch 59/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0306 - accuracy: 0.9932 - val_loss: 0.2171 - val_accuracy: 0.9758\n",
      "Epoch 60/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0269 - accuracy: 0.9931 - val_loss: 0.2007 - val_accuracy: 0.9762\n",
      "Epoch 61/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0266 - accuracy: 0.9938 - val_loss: 0.1818 - val_accuracy: 0.9777\n",
      "Epoch 62/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0277 - accuracy: 0.9934 - val_loss: 0.1803 - val_accuracy: 0.9772\n",
      "Epoch 63/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0245 - accuracy: 0.9939 - val_loss: 0.2105 - val_accuracy: 0.9753\n",
      "Epoch 64/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0299 - accuracy: 0.9932 - val_loss: 0.1902 - val_accuracy: 0.9770\n",
      "Epoch 65/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0281 - accuracy: 0.9941 - val_loss: 0.2277 - val_accuracy: 0.9760\n",
      "Epoch 66/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0260 - accuracy: 0.9938 - val_loss: 0.2206 - val_accuracy: 0.9771\n",
      "Epoch 67/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0282 - accuracy: 0.9934 - val_loss: 0.1876 - val_accuracy: 0.9779\n",
      "Epoch 68/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0297 - accuracy: 0.9937 - val_loss: 0.2021 - val_accuracy: 0.9769\n",
      "Epoch 69/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0267 - accuracy: 0.9941 - val_loss: 0.1776 - val_accuracy: 0.9782\n",
      "Epoch 70/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0308 - accuracy: 0.9929 - val_loss: 0.1896 - val_accuracy: 0.9773\n",
      "Epoch 71/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0258 - accuracy: 0.9939 - val_loss: 0.1900 - val_accuracy: 0.9771\n",
      "Epoch 72/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0269 - accuracy: 0.9937 - val_loss: 0.1988 - val_accuracy: 0.9751\n",
      "Epoch 73/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0246 - accuracy: 0.9942 - val_loss: 0.2193 - val_accuracy: 0.9772\n",
      "Epoch 74/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0306 - accuracy: 0.9939 - val_loss: 0.2568 - val_accuracy: 0.9751\n",
      "Epoch 75/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0302 - accuracy: 0.9935 - val_loss: 0.2598 - val_accuracy: 0.9766\n",
      "Epoch 76/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0285 - accuracy: 0.9942 - val_loss: 0.2733 - val_accuracy: 0.9777\n",
      "Epoch 77/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0244 - accuracy: 0.9945 - val_loss: 0.2490 - val_accuracy: 0.9787\n",
      "Epoch 78/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0327 - accuracy: 0.9934 - val_loss: 0.1914 - val_accuracy: 0.9769\n",
      "Epoch 79/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0241 - accuracy: 0.9945 - val_loss: 0.1976 - val_accuracy: 0.9762\n",
      "Epoch 80/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0259 - accuracy: 0.9944 - val_loss: 0.1826 - val_accuracy: 0.9760\n",
      "Epoch 81/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0270 - accuracy: 0.9940 - val_loss: 0.2081 - val_accuracy: 0.9776\n",
      "Epoch 82/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0263 - accuracy: 0.9943 - val_loss: 0.2278 - val_accuracy: 0.9757\n",
      "Epoch 83/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0242 - accuracy: 0.9945 - val_loss: 0.2246 - val_accuracy: 0.9757\n",
      "Epoch 84/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0228 - accuracy: 0.9946 - val_loss: 0.2181 - val_accuracy: 0.9773\n",
      "Epoch 85/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0302 - accuracy: 0.9937 - val_loss: 0.1962 - val_accuracy: 0.9763\n",
      "Epoch 86/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0254 - accuracy: 0.9943 - val_loss: 0.1848 - val_accuracy: 0.9770\n",
      "Epoch 87/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0243 - accuracy: 0.9942 - val_loss: 0.1518 - val_accuracy: 0.9772\n",
      "Epoch 88/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0234 - accuracy: 0.9949 - val_loss: 0.2325 - val_accuracy: 0.9743\n",
      "Epoch 89/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0323 - accuracy: 0.9941 - val_loss: 0.2068 - val_accuracy: 0.9780\n",
      "Epoch 90/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0227 - accuracy: 0.9948 - val_loss: 0.2171 - val_accuracy: 0.9767\n",
      "Epoch 91/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0239 - accuracy: 0.9945 - val_loss: 0.2044 - val_accuracy: 0.9794\n",
      "Epoch 92/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0256 - accuracy: 0.9944 - val_loss: 0.2145 - val_accuracy: 0.9754\n",
      "Epoch 93/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0241 - accuracy: 0.9947 - val_loss: 0.2448 - val_accuracy: 0.9769\n",
      "Epoch 94/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0284 - accuracy: 0.9947 - val_loss: 0.1760 - val_accuracy: 0.9783\n",
      "Epoch 95/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0239 - accuracy: 0.9948 - val_loss: 0.1605 - val_accuracy: 0.9781\n",
      "Epoch 96/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0262 - accuracy: 0.9948 - val_loss: 0.1852 - val_accuracy: 0.9778\n",
      "Epoch 97/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0224 - accuracy: 0.9950 - val_loss: 0.1858 - val_accuracy: 0.9765\n",
      "Epoch 98/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0200 - accuracy: 0.9956 - val_loss: 0.1922 - val_accuracy: 0.9774\n",
      "Epoch 99/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0235 - accuracy: 0.9948 - val_loss: 0.2114 - val_accuracy: 0.9773\n",
      "Epoch 100/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0263 - accuracy: 0.9948 - val_loss: 0.2063 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.273998</td>\n",
       "      <td>0.919767</td>\n",
       "      <td>0.143191</td>\n",
       "      <td>0.9590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.146305</td>\n",
       "      <td>0.956717</td>\n",
       "      <td>0.110218</td>\n",
       "      <td>0.9671</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.110638</td>\n",
       "      <td>0.966367</td>\n",
       "      <td>0.112754</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.098338</td>\n",
       "      <td>0.969767</td>\n",
       "      <td>0.115066</td>\n",
       "      <td>0.9684</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.085496</td>\n",
       "      <td>0.973917</td>\n",
       "      <td>0.099388</td>\n",
       "      <td>0.9723</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.026230</td>\n",
       "      <td>0.994800</td>\n",
       "      <td>0.185224</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.022351</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.185827</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.019978</td>\n",
       "      <td>0.995583</td>\n",
       "      <td>0.192168</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.994750</td>\n",
       "      <td>0.211363</td>\n",
       "      <td>0.9773</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.026262</td>\n",
       "      <td>0.994850</td>\n",
       "      <td>0.206273</td>\n",
       "      <td>0.9750</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy  epoch\n",
       "0   0.273998  0.919767  0.143191        0.9590      0\n",
       "1   0.146305  0.956717  0.110218        0.9671      1\n",
       "2   0.110638  0.966367  0.112754        0.9701      2\n",
       "3   0.098338  0.969767  0.115066        0.9684      3\n",
       "4   0.085496  0.973917  0.099388        0.9723      4\n",
       "..       ...       ...       ...           ...    ...\n",
       "95  0.026230  0.994800  0.185224        0.9778     95\n",
       "96  0.022351  0.995000  0.185827        0.9765     96\n",
       "97  0.019978  0.995583  0.192168        0.9774     97\n",
       "98  0.023545  0.994750  0.211363        0.9773     98\n",
       "99  0.026262  0.994850  0.206273        0.9750     99\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "import random, time\n",
    "\n",
    "seed = random.seed(time.time())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_test, y_train = to_categorical(y_test), to_categorical(y_train)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # Primera capa oculta con 128 neuronas y función de activación ReLu\n",
    "    # La capa de input tendrá tantas neuronas como características tengan nuestras\n",
    "    # instancias, en este caso 28*28 = 764 neuronas de entrada. \n",
    "    layers.Dense(128, activation='relu', input_shape = [X_train.shape[1]]), \n",
    "    # Capa Dropout para poner el 10% de los valores a 0 de forma aleatoria\n",
    "    layers.Dropout(0.1, seed = seed), \n",
    "    # Segunda capa densa con 128 neuronas y activación Relu\n",
    "    layers.Dense(128, activation='relu'), \n",
    "    # Capa Dropout para poner el 10% de los valores a 0 de forma aleatoria\n",
    "    layers.Dropout(0.1, seed = seed), \n",
    "    # Última capa oculta de nuevo con 128 neuronas y activación ReLu\n",
    "    layers.Dense(128, activation='relu'), \n",
    "    # La capa de salida, al tratarse de un problema de clasificación tendrá\n",
    "    # tantos nodos como clases existan.\n",
    "    layers.Dense(len(n_classes), activation = 'softmax')\n",
    "])\n",
    "\n",
    "# Utilizamos el algoritmo de Adam (descenso por gradiente estocástico) para\n",
    "# optimizar los pesos de los enlaces entre neuronas\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "# Compilamos nuestro modelo mediante compile()\n",
    "# - Usaremos CROSS ENTROPY como función de pérdida\n",
    "# - Usaremos el algoritmo de Adam como algoritmo de optimización\n",
    "# - Usaremos la exactitud y la exactitud balanceada como métricas del rendimiento.\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "                optimizer = optimizer, metrics = ['accuracy'])\n",
    "\n",
    "# Ajustamos los datos de entrenamiento utilizando 100 iteraciones del algoritmo\n",
    "history = model.fit(X_train, y_train, epochs = 100, validation_data = (X_test, y_test))\n",
    "model_history = pd.DataFrame(history.history)\n",
    "model_history['epoch'] = history.epoch\n",
    "\n",
    "model_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como se obtienen valores de la exactitud muy favorables tanto para el conjunto de entrenamiento como para el de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en conjunto de entrenamiento -> 0.998533308506012\n",
      "Accuracy en conjunto de test -> 0.9750000238418579\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos la exactitud tanto en el conjunto de entrenamiento como en el de test\n",
    "_, train_acc = model.evaluate(X_train, y_train, verbose = 0)\n",
    "_, test_acc = model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "print(f'Accuracy en conjunto de entrenamiento -> {train_acc}')\n",
    "print(f'Accuracy en conjunto de test -> {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al graficar el valor de la exactitud en los conjunto de entrenamiento y test en cada úna de las iteraciones del algoritmo podemos apreciar la evolución de ambas métricas. En este caso, vamos como ambas parecen variar muy poco a partir de las 20-30 primeras iteraciones y la exactitud del conjunto de test no supera a la de entrenamiento. Esto puede ser indicativo de que nuestras capas ``Dropout`` desempeñan de forma correcta su función, y no permite que el modelo sobreentrene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEVCAYAAADuAi4fAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABDSElEQVR4nO3deXgV5fXA8e/JCiFAQhLWsG8CgoCAICq4FdCKgtW6Vq2V/traaq22Wq0L1mqrte4LVlyriKi4oYIKuKLsyL4JJGELgUBC9nvP7493ktyEBC6QEJicz/Pkyb0z78y8M3Pvue+ceWdGVBVjjDH+FVHXFTDGGFO7LNAbY4zPWaA3xhifs0BvjDE+Z4HeGGN8zgK9Mcb4nAV6c0SIyG9EZJuI5IpIUl3Xp66IyNUi8lUtL2OZiAyv6bLm2GWB/iggIrNEZJeIxNZ1XWqDiEQDDwM/UdV4Vc2qgXluEJGzamA+tR54wyUiHURERSTqcOajqr1UdVZNlz0SRORFEfl7XdfDbyzQ1zER6QCcCigw+ggv+7ACykFoATQAlh3shOLY59RzBPeZ8RNVtb86/APuBL7GtXg/qDSuLfA2kAlkAU+EjLsOWAHkAMuB/t5wBbqElHsR+Lv3ejiQDvwF2Aq8AiQCH3jL2OW9Tg2ZvhnwArDZGz/VG74UOC+kXDSwA+hXaR26AXu9euUCn3vDTwbmAru9/yeHTDMLuM/bLvmh6+ONfwUIeuNygT97wwcD3wDZwGJgeMg0VwPrve31I3A50AMoAALefLKr2UfXhGzr9cCvQ8aVbtM/AduBLcA1IeOTgPeAPcD3wL3AV9UsZ1PIdsoFhnj1/hr4j/cZ+DvQGfjce78D+B+QEDKfDcBZ3uu7gcnAy179lwEDDrFsf2ChN+5N4A28z1YV69IFmO3t3x3AGyHjjgNmADuBVcDF3vBxQDFQ5K3/+97wvwAZ3nJXAWfW9ff2WPur8wrU9z9gLfBb4ETvQ97CGx7pBav/AI1wLeJTvHEXeR/8gYB4X6r23rgDBfoS4J9ALNDQC0QXAnFAY+8LPDVk+g+9L3QiLpgP84b/udKX93zgh2rWsYNXryjvfTPcj8aVQBRwqfc+yRs/Cxf0ennjo6uYZ1mA8t63wQW+c3BHqmd771O87bcH6O6VbQX08l5fTTWBN2Te5+KCqwDDgDzKf1hLt+l4b/uc441P9MZPwgXPRsDx3n6rLtBX2E4h9SsBfu9ti4be/j7b24cpwBfAI1VtG1zwLvDqFQncD8w52LJADLARuMFbz7G4gFxdoH8duN3bF6Gf3UZAGu7HMwroh/sh6Fn58+q97+6Vbx2yjTrX9ff2WPur8wrU5z/gFFxwT/berwT+6L0egmtlR1Ux3SfADdXM80CBvghosJ869QV2ea9b4VrOiVWUa41rYTXx3k/Ba1lXUbZCAMMF+O8rlfkWuNp7PQsYf4BtVxagvPd/AV6pYjtd5QWXbNwPWsNKZa7mAIG+imVPLd3+3jbNp2Jw3o47uoj09u9xIeP+Ud3yKm+nkPptOkB9LgAWVrVtcMH705BxPYH8gy0LnIb7kZKQ8V9RfaB/GZhAyNGhN/znwJeVhj0L3FX58+q97+Jtz7Oo4gff/sL7s9xn3boKmK6qO7z3r3nDwKVtNqpqSRXTtQXWHeIyM1W1oPSNiMSJyLMislFE9uBahwkiEuktZ6eq7qo8E1XdjEspXCgiCcAoXAohHK1xrcNQG3Gt8lJp4a6Qpz1wkYhkl/7hfkhbqepeXID5P2CLiHwoIseFO2MRGSUic0Rkpzffc4DkkCJZlfZTHhCPa21HVVqXyusdjgrbQkRaiMgkEcnw9tmrlepT2dZKdWuwn1x/dWVbAxnqRd+q6lXJn3FHQN97PXt+6Q1vD5xUaT9dDrSsaiaquha4EfcjtN1b79b7Wa6pggX6OiIiDYGLgWEislVEtgJ/BE4QkRNwX6J21Xwh03CphKrk4dIwpSp/gbTS+z/hDo9PUtUmuJYbuC9pGtDMC+RVeQm4ApdK+lZVM6opV9lm3Bc+VDtci7G6elZWeXwarkWfEPLXSFUfAFDVT1T1bNxRykrguXCW4/WEegt4CJdWSwCm4bbPgWTi0i5tQ4a1O4h1qm74P7xhvb19dkWY9TkcW4A2IhK6nLbVFVbVrap6naq2Bn4NPCUiXXD7aXal/RSvqr8pnbSKeb2mqqfgPjOKSz2ag2CBvu5cgDsJ2BOXLumLOzn4JfAL3Im7LcADItJIRBqIyFBv2v8CN4vIiV6vlC4iUho4FwGXiUikiIzE5ZT3pzEu9ZAtIs2Au0pHqOoW4CPclzRRRKJF5LSQaafiTtDdgDtUD9c0oJuIXCYiUSLyc287fHAQ89gGdAp5/ypwnoiM8Na9gYgMF5FUrwV8vog0AgpxJ/qCIfNJFZGYapYTg8uFZwIlIjIK+Ek4FVTVAO5k+t3ekVNPyo/YqpLp1avTfsqA22e5wG4RaQPcEk59DtO3uM/r9d4+Ox8YVF1hEblIRFK9t7twATqI28fdRORK7/MULSIDRaSHV7bCfhWR7iJyhveDW4D7rAYxB8UCfd25CnhBVTd5rZ+tqroVeAJ3KCvAebgc5SZcz46fA6jqm7heKa/h8uRTcSc4wQXd83A56cu9cfvzCO4E3w5gDvBxpfFX4vLMK3G50htLR6hqPq612xEX0MKirh/9T3FHE1m4w/yfhqSwwnE/cId3+H+zqqbhTgj/FRcw03ABMML7uwl3JLET9+NX2oL8HNe7ZKuI7LN8Vc0B/oA7oboLuAzXiyZc1+PSOFtx+ecXqiuoqnl4vY289RpcTdF7cD+wu3Eny8Pe9odKVYtwJ2CvxX22rsAF7cJqJhkIfCciubjtdYOqrve250+AS3D7YyvlnQMAngd6eus/1Rv+AO7zuRVoDtxW0+vnd1Ix5WbMwRGRO4FuqnpFXdfFHFki8h3wjKpW++Nljg7WojeHzEv1XIvrXWF8TkSGiUhLL3VzFdCHfY8AzVHIAr05JCJyHS498pGqflHX9TFHRHfctR3ZuLTbz7zzOOYoZ6kbY4zxOWvRG2OMz1mgN8YYn7NAb4wxPmeB3hhjfM4CvTHG+JwFemOM8TkL9MYY43MW6I0xxucs0BtjjM9ZoDfGGJ+zQG+MMT5ngd4YY3zOAr0xxvicBXpjjPG56p4EX2eSk5O1Q4cOdV0NY4w5psyfP3+HqqZUNe6oC/QdOnRg3rx5dV0NY4w5pojIxurGWerGGGN8zgK9Mcb4nAV6Y4zxOQv0xhjjcxbojTHG5yzQG2OMz1mgN8YYn7NAb4wxR0hhSYB3F2WwYNOuI7rco+6CKWOM8ZuC4gCT56Xx9Kx1bNldgAiMO60Tfzq7OzFRtd/etkBvjDnmBYNKRISEVTa3sIRnZ69j6+4Cbh11HEnxsWXj0nflMX/jLvq3S6Rts7gqp1dVNmblkVtYQlEgSFFJkPyiAHlFAfKLAxSWBNyw4gCbsvJYtS2H1Vtz2FsUYED7RO4bczwzlm/n2dnr+XL1Dq4Y3J5deUXs3FtEUnwMvx3epUa2SShR1Rqf6eEYMGCA2i0QjPGX3XnFLMnIZtGmbApLgvz29M7ExZS3M9N25jHhi/VER0aQFB9D88axDOmcRGpiebAtCQRZunkPq7fmsC4zl3WZe9myO5/tOYVk5RYyrFsKD110QoXAvWtvEVv3FBAfG0XjBlF8tHQr/56+ih25RURFCAlx0fzzwj4M6ZzEM7PW8ewX6yksCQLQISmOkzom0SaxIc0bxxIdGcE367L4Yk0mmTmFYa13s0YxdGsRT7cWjRnZqyVDOich4n6Qpi/byq1v/8DOvUUAxMVEMqhjM168ZtAhbWMRma+qA6ocZ4HemGNbSSBIXnGAJg2iKwzfuruAD3/YQqeURvRvm0jTuOhq5gBFJUG+XJPJjOXbGNOvDSd1SqowvjgQJBBUGkRHAq4FPWPFNv775Xqy9hYxoH0igzomMbRLEq2aNiybLjOnkPEfLOf9xZvLholAv7YJvHD1IJrGRbNmWw5XPP8du/KKiY4Q9hYFysr2bNWEU7sls277Xr5bn0VOYQkAMZERdEiOo01CQ1o0aUCD6Ehe+34TiXHRPH5pf9okNmTC7HVMmptWFrhLDWifyB0/7UmD6Aj++MZiVmzZQ0JcNNl5xYw+oTVXndyeJem7+WrNDhZs2sWuvOKyaRPiojm1awpDOyeR2CiGmKgIYiMjaBgTSVxMFHExkcRGRRDj/YX+mFVlb2EJO/cWkRwfS8OYyP2WPRAL9MYcBfYWlvDOwgziYiI5sX0i7ZrFlbXuDmT7ngL+8+lq1mXuZXj3FH7SsyWNG0Tx+vebmPR9Gpm5hVw5uD03ntWVhLgY3l+8mTumLmV3fnmQ6tYinrN7tuCnfVpzXMvG7Cko4dt1WcxenclHS7eQnVeMCMRGRfDiNYMY7AX7uRt28ptX55OdV0zXFo3p2aoJi9J2sS5zL22bNaRb88bM27iL3flu+lO6JHPRgLbkF5Xwj2kryS8KcM3QDpzWLYXeqU35Zu0O/vD6IjqlNOLPI7vzp8mLiYqM4JVrB3FcyybkFwVI35XHzFXbmb5sG/M37aJNQkNO7ZrM0C7J9G7TlNTEOCIrpWqWbd7N9a8tZGPWXiIjBFUY278Nw7o1Z29hCTmFJbRrFsdZPZqXbffCkgCPfrqGBZt28aefdGdgh2b7bPvCkgCZOYXkFQXonBK/z3KPFhboTb2hqmEHz5qWV1RSZQsuvyjAK3M28Mzs9WWH6QDJ8TEM7NCMkzsnMaRzEk0aRrN9TyHbcwoIBF3rsWnDaD5ZupWnZ6+jOBCkc0o8K7fmAK5lrAqndUuhVZMGvDk/jSYNo+nbNoFZqzI5oW0C94/pze78YhZs2sU363YwZ/1OAkGlZZMGbM8pIKguZXBWjxac37c1vVo35crnvyMjO5+XfjmIjVl5/PXtH2id0IBzerdi6eY9LN+8m5ZNGzDutM6cc3xLoiIjCAaV1dtz+OiHrUyZn05Gdj4Agzo04x9je9OleXyFbfLlmkx+/cp88ooCpCY25NVrT6JDcqMqt2tBcYDYqIiw9mtOQTH3f7SSmMgIrjutE20SGh5wGr+wQG/qha27C7jo2W9o36wRd53Xk64tGh/SfFSV7TmFrNyaw9KM3SzN2M3WPQV0TG5E9xaN6ZjciLiYKGKiIsgvDvDl6kw+X7Wd9Zl7GdmrJbef24O2zeIoLAkw6fs0Hv98LTtyCzmtWwo3nNmVRrGRLNiYzbyNO/lu/c6yoLg/I3u15NZRx9EhuRFbduczY/k2du4tYky/NrRPcgFyxZY93PP+MuZu2MUNZ3blt8M7ExVZsUdHVm4hHy3dytdrd9C1eTyndE2hb9uECj0/tucUcMmEOaTvyqeoJMjJnZN46vL+JMTFhLX9gkHl2/VZ5BaWcHaPFtWeJF24aRevztnELSO607Jpg7Dmbapngd4c1YJBZcuegipbX+G20AuKA1z87Les255LZISQVxTg6pM7cO2pHWnZpMEB5xEMKi9/u4F3FmawLnMvuV4uGKB9UhwtmzTgxx172V7FSbiYyAgGd06ic0oj3pibRklQ+dmJqcxelUlGdj4ndWzGzSOqTguoKmk785mzPovCkgApjRvQvEksURFCdl4x2fnFtE1sSL92iQfcBqXz21sUID728DrUbdtTwLhX5tOvbQK3n9uD6Ei75OZoZ4HeHLVUlT+9uZi3F2RwTu+W3DqyB+2S4tiUlcdjn6/hwyVbuHlEd345tEO1wVpVuWHSIt5fspkJVw6gf7sEHpq+iklz01CFxLhoerRqQkxUBFuyC9i8O58WTRpw1ckduLB/G3bkFHHzlMV8/+NO+rZNoG/bBDqnNKJz83h6tW5K04blJzGz84rYtDOPgmLXrU4E+rZNoJEXWLfuLuCfH6/knYUZ9Eltyi0junNKl+Q6SyeZ+sMCvalz2XlFPPLpGtZsz+HOn/aie0uXVrn/oxU8O3s9ZxzXnG/XZREIKkO7JPHlmh1ERgjHtWzM4vTdjO3fhn+M6V3W66NUYUmAp2au49HP1nDLiO787vTyPsirtuYwZ30WK7bsYcWWPQRUadW0Ia2aNmBRWjZL0nfTpEEUJUElUoS/ndeTi05MrZGgvKegmMaxURbgzRGzv0BvF0yZWpVfFGDK/DT+PWM1e/KLiY+N4qePf8kNZ3YlOjKCZ2ev58rB7Rl/fi+27SnkoemrmLF8G1cMbs9vhncmJT6WJ2au5eEZq1m9LYdh3VKIi4lCBOZv2MU367LILw4w+oTW/HZ45wrL7t6ycdkPSmWqyoJNu3jh6w0o8NdzetToibvKXR2NqUthtehFZCTwKBAJ/FdVH6g0vj0wEUgBdgJXqGq6N+6fwLle0XtV9Y39Lcta9HVjT0ExnyzdygltE+gWxknM5Zv3cMfUH8jaW0S3Fo3p1iKeSBE2ZOWxIWsv2/YUkJ1XXNaHeUinJO4a3ZOU+Fjuem8ZHyzZAsA5vVvy+KX9D9hlbcbybdwx9Qd25BYRCLrPbLtmcQzvnsLw7ikM69b8qO32ZsyRcFipGxGJBFYDZwPpwFzgUlVdHlLmTeADVX1JRM4ArlHVK0XkXOBGYBQQC8wCzlTVPdUtzwL9kVUcCPLad5t49LM1ZV3/BnVsxhWD2zO0c1KFqwzBXZzzzGyXKmnaMIaBHRJZsz2XDTv2ElQlNTGO9klxtGragMS4GJp6+fHh3VIqpDE+XrqFeRt2cfOI7vukY/ZHVSkKBCksCVqr2ZgQh5u6GQSsVdX13swmAecDy0PK9ARu8l7PBKaGDP9CVUuAEhFZAowEJh/sSphDk5lTyOK0bDKy8+mT2pTebZoSFRnB5ux83lu8mUnfb2JDVh5DOiXxhzO7siQ9m/99t4k/vL4QgKRGMXRpHk9UpJBbUML2nEK27C7gp31ace/5x5PYyHW5K/Ja7uHeoGnk8a0YeXyrg14fESE2KpLYqMO7itCY+iScQN8GSAt5nw6cVKnMYmAsLr0zBmgsIkne8LtE5N9AHHA6FX8gTC35dPk27npv2T59tONjo2jXLI7lW9xBVd+2CUw8ryend3dXCw7pnMR1p3bi+w07WZqxmzXbclmbmUtJUEmIi6FtszjO6d2Kc3pXDNJH4g58xphDU1MnY28GnhCRq4EvgAwgoKrTRWQg8A2QCXwLBCpPLCLjgHEA7dq1q6Eq1V8/pO/m+tcX0CGpEXec24MT2ibQOqEhCzbu4tv1WazdnstNZ3fj/L6tyy62CRURIQzulFR2Cbwx5tgWTo5+CHC3qo7w3t8GoKr3V1M+HlipqqlVjHsNeFVVp1W3PMvRH56tuws4/8mviIqIYOrvhpLSOPbAExljjnmHm6OfC3QVkY64lvolwGWVFpAM7FTVIHAbrgdO6YncBFXNEpE+QB9g+iGvST1WWBJg1dYcNmTlsXHHXooDQdonNaJDchzJ8bEE1Z1YvWnyInILSnjrtydbkDfGAGEEelUtEZHrgU9w3SsnquoyERkPzFPV94DhwP0iorjUze+8yaOBL73eFntw3S5LKi/D7F9+UYCLnv2GpRnlnZUiBIJVHIxFCPz3qgEc17LJEayhMeZoZlfGHkWCQeX1uZvomNyIkzsnA6474U2TFzN1UQb3nn88gzo2o12zOCJESN+Vx8asPLL2uocoREYIHZMbcXybpnW8JsaYI82ujD0G7NxbxB/fWMTs1ZmIwB/O6MofzuzKq3M28s7CDG46uxtXDG5fYZpOKfF0SomvZo7GGONYoK9jJYEg3/+4k5vfXMyO3CLuGd2LJem7efSzNXy1dgeL07I5q0dzrg+5h4sxxhwMC/R1oKA4wKOfreHbde6GW4UlQdo2a8hbvzmZ3qlNUVUGdUzkb+8uIzWxIf++uG/YDz42xpjKLNDXsqKSYIWLibJyC/nVy/NYlJbNwA7NuHJwe45v05QzejQvu6RfRPj5wHYM7ZJMXExUhdvkGmPMwbJAX4uWpGfzs6e/5bhWjRnbrw29UxP44xuL2LangKcu68+o3vu/BUBqYtwRqqkxxs8s0Neif368krjYSEoCyt3vuzs/JMfHMGnc4LCfGGSMMYfLAn0N+GbdDv7y1hIeu6RfWQD/ck0mX6/N4q7zenLN0I6s2LKHb9Zl8ZOeLWjbzFrqxpgjx+5EVQMembGGtJ35jHtlPpuz8wkGlX99vIo2CQ257CR3754erZpw7SkdLcgbY444C/SHaeGmXXy/YSeXn9SO/KIA1708j7cWpPNDxm5uOrub3U7XGFPnLNAfpglfrKdJgyj+ek4PHru0L8u37OGWKUvo1iKeC/q1qevqGWOMBfrDsTFrLx8v28oVg9vTKDaKM45rwe3n9EAE/jLyOHu0nTHmqGAnYw/Df7/8keiICK4+uUPZsF+d2okL+6eWPXnJGGPqmrXoD9HOvUW8OT+NMf3a0LxJgwrjLMgbY44mFugP0ZMz11JYEuRXp3as66oYY8x+WaA/BMs27+bFbzZw6aB2dG3RuK6rY4wx+2WB/iAFg8odU5eS0DCav4w4rq6rY4wxB2SB/iBNmpvGwk3Z3H5uD5rG2c3GjDFHPwv0B2FHbiEPfLSCwZ2aMcb6yBtjjhFhBXoRGSkiq0RkrYjcWsX49iLymYgsEZFZIpIaMu5fIrJMRFaIyGPiPUD2WDN/4y4ufPobCoqD/P2C4zlGV8MYUw8dMNCLSCTwJDAK6AlcKiI9KxV7CHhZVfsA44H7vWlPBoYCfYDjgYHAsBqr/RFQVBLkwU9WctEz31ASUF65dhBdmtsJWGPMsSOcC6YGAWtVdT2AiEwCzgeWh5TpCdzkvZ4JTPVeK9AAiAEEiAa2HXatj5DiQJD/e3U+n6/czkUnpnLneT1p3MDy8saYY0s4qZs2QFrI+3RvWKjFwFjv9RigsYgkqeq3uMC/xfv7RFVXVF6AiIwTkXkiMi8zM/Ng16FWqCq3v/MDn6/czr0XHM+DF51gQd6Y+iRQApsX1XUtakRNnYy9GRgmIgtxqZkMICAiXYAeQCrux+EMETm18sSqOkFVB6jqgJSUlBqq0uF5eMZqJs9L5w9nduXKwe3rujrGmCPt6//AhGGQubqua3LYwgn0GUDbkPep3rAyqrpZVceqaj/gdm9YNq51P0dVc1U1F/gIGFITFa9NL32zgcc/X8slA9vyx7O61nV1zJGyYy2snl7XtfC33emw5E1Qreua7F9JEXz/nHu9+qO6rUsNCCfQzwW6ikhHEYkBLgHeCy0gIskiUjqv24CJ3utNuJZ+lIhE41r7+6RujiYTv/qRu95bxlk9Wljvmvrm/Rtg0qWQe3SkD+vE3iwIBmpn3unzYMJwePtXsPj12llGTVn2DuRug5j4/f/4BwOw9G1Y9RHs/BGCwSNXx4NwwECvqiXA9cAnuCA9WVWXich4ERntFRsOrBKR1UAL4D5v+BRgHfADLo+/WFXfr9lVqDnPzl7H+A+WM7JXS566vD9RkXaZQY3atgzWz9p/mR1rYfrfoDCn4vCCPTDjTti5vuLwkkKYcResmXH4ddv4FQRLYPFrBy6ftxM+vRueGgK7NlQcpwrrZ0PR3sOrU20pKYIV70NhbsXh62fDw8fBC6Ngd0bV0x6qpW/Di+dCTCNo3R8+vhVyttbsMg7VlsUw+0GXkwe3/+Y8BcndYNA42PQt5O+qetrpd8CUa+D1S+CxvnB/Krw0Gr54EDZ9d9QEftGj7BBqwIABOm/evCO+3Be//pG731/OeSe05uGLTyD6WAzy+dmwYw20HVh7y8hY4L6gjZLdX0IHiKi0rVZPhx9nQ0p3SOkBezLcYfDGr9z4oTfCmXftO13WOnjhHMjdCoN/CyPvLx837c/w/bOQ0B6unQGNW7jW1JRrYPm7rky/K2HEPyAyxrXIFr4CcUkw6DrocCrs7+js/RtdKzOpK5Tkw/Xzqi5fUgRf/AvmPO0CuUTAwF/BOf8KWf9P4LWLoeMwuPxNiIoNc+MewBcPwtrPof8v4PixhzbfrUvhnf+DbT9Ay95w6RvQtI0Ldi+c6/bp3kw377HPQZczD7/eXz/qfqTbDYGf/88FzWeGQpez4Oev7n+/FOfDt0/ACZe5etaG/10Ea6ZD/6vgvEch7TuYOALOfdhto+fPhp9NhOMvrDjd3Ofhw5tg0K+h989g+wrYthQ2fuP+A/T5OVzwTMXPekkRBArd0UINZgxEZL6qDqhynAV6d/+akx/4nI7JjXjl2kHHZks+GICXzoONX8OpN8Ppt+8bSA9XwW54qLsLhKXanwKXT3YtNYB1M+HVC0GDuN61noR2LiDu/BHmvwA9zoMxEyDGe4buzvUu0AQKoe1glxe9bia07ut+XJ47A7r+BDZ8Cc06wzUfwmf3wtzn3I9GYQ58/Qg0bgUlBZCX5YJ2Xhbk73Q/OIOuc1+82PiK65WfDQ/3cMGz/VCY+hu4ehp0GLrvNpjztGuN9jwfht/mgtjy9+Cm5dAwwZV5foT70hfuhl5j4MKJ5fuiOB8yV0HmSlcmtNXf6wLocErV2375uzD5F9AgAQqyIS4Zhv4Bht6wb9lgcN99Hwy47TPzflfPk34NXz3q9tuof8K0myGqAVw73dXpjStdHTueCs17QfMebp/FNas43x+/gECRC9qVqbqjnq8fgV5jYcwz5T9OpcH/Zy+47V6dD26Cec9Dp+Fw5dSqA2OgGN79HSR1cZ/90HUPBtxna/tyt72b93D7rlTBHniwM8S3gN1pcNqfYccqd3Rz03K3TR7q6tZv7ITy6Uo/513OhEsnQUSlR4buzXJHBV8+BIN/ByPuc3X/8Qt461cuLRTVwO3Hwf8HJ/+++m0Qpv0FenvwCLA4PZutewr488juhx/kVd2HqnnPmvu1VoXVH0Ns4+oDwbdPuiDfdrD7cGWtdV+s6IY1UweApW+5IH/h89CgqWu1fDYeXvs5XDbZnWibfJVryV/zkQuw21dAZCx0Pt19GVQhuSt8cjtsHeICcKMkWDfLBeir3oemqfDEQPjgRvjldPc/vgVc+BykzYXXfw5PnQx70uHkP8Cp3iUc3c9xh9JxSXDSONeiLilwaYPvn3Wtr0/vgX6Xu0CX2MFNt+g1KM6Dgde5un30F1jwctWBftlUaNEbLn7ZvT/p/9yRwMJX4eTrYeO3kDYHRj3olj3jb+7L3GGoq8ea6W44QES026fghi18Fa6ZBm36V1xm5iqY+ltoM8CN3/RteaBMOQ66jSgvO28izLgbRj/mfjjABcJ3fu32X88LXEu1URJ0P9ftuzevgoaJcMXb0KS1m+a6z2DW/bDhK1jwkts+C19x+6M0kO7dAZMud/P//fyKLe5gAD78k/tRH/BLOOehisFw8O/ctpx2s1t259P33dbL33VBvmVvl/Jb+pZrOVc28z5Y8oZ7vW1Z+ed+1cfuR3nXj+VloxtBp9OhQRP3fs1090M19jlY9D93tAbuqLO08dLlbFcuGHDrsGNt+ef8wuf3DfLgtu8Zd0BRLsx50h0padDVtVlnGPwb1wjZNMd9JnuMhsRa7N2nqkfV34knnqhH2v3TVmjn2z7U7L1Fhz+zH6ao3tVEdcmb4U+z6hPVzNVVj8vNVH3jSjfPB7uqlhTvW2bLD6rjk1Vfv0w1GFT9+jHVu5qqPjvMjatOcaHq9/9VTZ/vpjuQCWeoPjm4YtnFk1XvTlB98aeqj5yg+q/Oqjs3HHheqz5RffkC1aeGqj7YTfWRPqqbF5ePX/KmW+fnR7j/P7wVssw33LC3f60aCBx4WaquzhvnqL75S9V7mqne20J1zjOqgRLVR/upPndWedn3/6h6b3PVvJ0V57E7wy131r8qDp84SvXh492+efVnqv/spFq41437+K9umruaqP6ri+oHf1Jd+o7q9pWqJSGft5ztqv853m2L7PTy4fm7VR870W3X0OHFhapPDHLLLcx1w7avdPW+t7lb3hcPuXq8+jP3/suH990uOdtU37vBfQaqEwiozp3o5rHwf+XDP7hJ9e5E99l7+9cVp3n/Rld+xt3Vf7a2r3L7/a4m7jO+a1P5uJ0/qv6jreqE01WL8lWfHe62X96uivNY86mb/t3rVb961H3uJ5yu+upFbvjjA1Tnv6yasUD1xy/dsO+fK5/+jV+471Ug4Pbfa5e47ZedVl6m9Du9cY5qQY7b7g90CO9zHgiovnlN+WfgzWtUC/aUj9+doTo+RXXqbw88rwMA5mk1cbXOA3vlvyMd6IPBoJ7+4Ey9/Lk5BzdhcaHqni2VZ6b6zKluhz7Sx5U5kD1b3Ifzvjaqq2eUDw8E3Afsn53cF2nyVW6+Kz+qVI8C1SeHuC9Bbmb58BUfuA/j3QmqH96y7xdE1QW60g/gI31Up/9NdcGrqqs+Vt28qOIXdNsKV+7rx/edz8LX3DqMT1Hd9N2B1zkcwaDqS6PdMl8Zu2+wyE4LP8hXlp2m+sqFbt5PnOT+L55cPj5joRv23YSK08151g3fvqri8GXvuuGf3+f+zw75IQgEXKBZP9v9qOzP1mXuc/D0UPejN+ufqo/1d8H0xy/3Lf/jV2550+90PxrPnFYegN78ZXnj4K6mLlAfjkBA9bkz3ecsP9t9Hu5OVP3wZrf8u5q4YKqqOv8lr15/O/B8i/LdD+e9LVTvSXLrPuVXqk+d7AL9zh9duc2L3Gf5/RvLp92z1f0APnFS+Q/r8vfcvO5r7Ro8od/B0u/nk0Pc66J81b+3cj90ZetZ4uYbKm+XW9cZd7vv4d0Jqms/D3/bFRe6H8W5E6v+0Zv2Fzf/HWvDn2cV9hfo632Ofs22HM7+zxfce34vrhzSYf+Fi/LcYfOyd1yvhaJcl9Nsc6Ib/+MXLk/ea4wrM+pBl0LYn++fc4evzTq7Q8yR/3Spi1n/gK0/QMs+7lA0uZvLI7cb7E5glfrsXpequWxyxUN4cD1DZv7DHf7GJcN1n0OCd0lEcYHrJZDYAfpd4dIK62eBhnStG3gdnPuQez39DpefvmklxFdxUduaT13+teM+18Mdul0bXGrozLtq/rBW1aVnPvmrO0S/cSlEhTwC8tnT3KH6r78sT1W8cK473P7dnIrzCgbctszeBDGN4Y9Ly/P1B2v1dJeaUq+3RrshMOR66PHTqstP/R0smeRy4D9MhotfgZ6j3frNesCdyBz92L4nEg/F5oUw4XR3ojxrjetV8oeFEBkFj/Vzabif3AsTR0L7IS4VVFVaoyrZm2Duf13qZfsKyNkCF71YMZ/+8W3uM9jjpyCR7hzCro0wbqbLvYfOKzrOpUsqm/+i60b7y+kutfj6Ja6eBzrp/MK5kDHPpdjOuhtO+WN46xWOnG3w6AnuHMiFzx3ybPaXo6/zFnzlvyPdon/8s9Xa/i8f6Nbd+VUXWDdT9bVLXVrirqaupXJfG9W3rlN96DjXmigucGVfudC1MIryVV84170OPUyrygvnqj4+0B0SvnZJeQv70b6qiyZVbAV+/FfX6iltue/c4FrRU361/2VkLHAtl5fHlLcovn/OLWfdzPJyhXtdCyptbvmh98LXXGvxX51dashv9myp+hB84Wtu/ec+797nbHP7//P7qp7P14+78p/ccfh1Wvau6rdPVUzVVCd3h2vF39VE9e3/23f8oR71VOfd35d/D756tHx46efpH6mqD/dy9TocVaUoC/a4lMzjA93fk4MrpvTCUZDjfX/Hqb7zG3fUEM6R91ePuvWbdHl4ac6DNf1vbrtuW3HIs8BSN9U797Ev9IInv6p65A9TXD733z1cLm/mA6orp6kW5bnxKz8qP2Tfuqxi/jZtrns/8/7qF56z3R0GfvZ39z5Q4g43F7xa9Qe9dBnfPOnev/nLffOJ1SlNOyz8n/tg/7un6n/Prv5DW1LsfoTube7W+64mqis+PPBy/CIYVH3xPBcUdm1y5zLuaqK6dWnV5QtyVD8dr7o368jWU1V1+fsuvZWfXfvLys1Uvb+t6n96lzdwVN3n5fGBruFRmsI5Wn1wk6vn/W1dgy0cuTtUP73HnTOpDbk7XLrpjV8c8iz2F+jrda+b9F15LM3Yw62jqngk4LwX4IM/ukPnyya5XiaVdR/puut9+W/Y8LU7XBx4rRuXOsAdin3zuEtpNEpxXf86DS8/nF35gTtE7+lddxYRuf9uVi16uotNFv0PUgfC0ilw2i0u1XMgA38Fy952vRCy01yPldGPVt8zKDLKdX2bMMylkRqlQNezD7wcvxBxKY+nTna9fgLFrvte88p36PbExsOZfzuiVSzT46fVp3ZqWqNkdx1DdFzFfvyRUXDlOy611arPkanLoRrwS5cmChTCcWFut0ZJcOadtVenRklw2s0uPaxao/3roZ4/YWr6MnfH5BG9WpYP3JvlcoEf3OgC2xVvVR3kS418wHUP2/iVy3WH9jM+826IbeL6Er/7O3h1LEy7pXz88nehWSdocXz4le53uevW+Na1rsvh0BvDmy4iAkY/4a4knfUPd16h8wHykvEprhthZKy7SCeynt29M7EDnHUXrP3UXQDW8/wa/wIek1K6l5/rCdW0zdEf5AFa9HLdkKMa1swFYTXllD/CGbfXymesfgf65Vvp1iKejsmN3NV6n42HR/u4Ez4nXuOu4iu9oKc6cc3gvMfcydQhv6s4LrkL/GkF/HUz3LDYXUE373kX4PN2upO3Bxs8jr/QBd7sje6iqMoX/+xPchc4/a/u9bBbw1tu6gB34cjpd4S/HD8ZeJ0LClDxxKA5tp3/BFz6WnlfeZ+rt6mbnIJi5m3YxXWndXKt+Ikj3EVGvcbA8FtdqyVcx50D3UdVHzhjGrm/n/zdnbl/9/fuajgNHHzwaJgIfS9zPXL6XXFw04K7wKjnBQfXi6Wq3gv1RUSEu/x97QzXA8r4Q3JX91dP1NtA//XaHZQElTM6xsFrF7nLn696/9C7B4bTOo6KcVfSPXsazP6nuy1Aq74Hv6zzHjn0PJ5I7V6B50dN28CJV9d1LYw5ZPU2dTNrVSaJsXDi9ze6/sE/e6Fm+4BXp1lHd5IPDi/na7liY0yY6mWLXlWZvTqT/zR9g4h1n8Hox1365UjpNcalYA6lNW+MMQepXgb61dty2bl7D6c0mu7y3P1/ceQr0Wn4kV+mMaZeqpepm1mrtnNyxDKiAvmudW2MMT5WLwP97NWZ/Cz+B3fj/w5HIC9vjDF1KKxALyIjRWSViKwVkVurGN9eRD4TkSUiMktEUr3hp4vIopC/AhG5oIbX4aDkFpYwd0MWp+l8dw/smnr6jzHGHKUOGOhFJBJ4EhgF9AQuFZHK14E/BLysqn2A8cD9AKo6U1X7qmpf4AwgD9jPk3ZrSchzG79Zu4PuwfU0Ls50D6owxhifC6dFPwhYq6rrVbUImARUvsqnJ/C593pmFeMBfgZ8pKp5h1rZQxIods+n/NhdETprdSajYhahiHs0nTHG+Fw4gb4NkBbyPt0bFmoxUPrgxzFAYxFJqlTmEuD1qhYgIuNEZJ6IzMvMzAyjSgdh9Sfu0X5zniS48iNmrtzOebGLkLaD6vcVn8aYeqOmTsbeDAwTkYXAMCADKHuChYi0AnoDn1Q1sapOUNUBqjogJaWKh1ocjgUvubtGtjiewNTfkbhnJe2K1rpbFhhjTD0QTqDPAEJvVZfqDSujqptVdayq9gNu94ZlhxS5GHhHVYsPr7oHaXe6u/Ng38th7ASkcA+vxtzvxnWzQG+MqR/CCfRzga4i0lFEYnApmPdCC4hIsoiUzus2YGKleVxKNWmbWrXwf+5+7/2vpCS5B09wCc0kBxI7HtxNy4wx5hh2wECvqiXA9bi0ywpgsqouE5HxIuI9MYPhwCoRWQ20AO4rnV5EOuCOCGbXbNUPIBiAha+4K1ATO/DNuiweyx/B5nbnudsJ271ijDH1RFi3QFDVacC0SsPuDHk9BZhSzbQb2Pfkbe1bN9PdkfLs8QC8t3gzjWJjaHblSxAd5gOLjTHGB/x7ZeyClyAuCY47l4LiAJ8s3cqI41vSwIK8Maae8WegLymEVR9B74shKpbZqzPJKSxh9Amt67pmxhhzxPkz0O/8EYLF0KY/4NI2SY1iOLlz5a79xhjjfz4N9Ovc/2adUVW+WJXJT3q1ICrSn6trjDH748/Il+UF+qROZOcVk1NYQpfmjeu2TsYYU0d8GujXuhOxDRPJyM4HoE1CwzqulDHG1A1/Bvqd66FZZwDSd7lAn5pogd4YUz/5M9BnrYMkF+itRW+Mqe/8F+iL9kLO5rJAn74rj0YxkSTERddxxYwxpm74L9DvXO/+e6mbjF35tElsiNgtD4wx9ZT/An1Zj5vy1I2lbYwx9Zn/An1IH3rwAr2diDXG1GP+C/RZ6yC+JcTGk1tYQnZeMamJcXVdK2OMqTP+DPRJ5fl5sB43xpj6zX+Bfuc6aNYJgIxs9xxyS90YY+ozfwX6gt2wNxOSugAhF0tZi94YU4/5K9BX7nGzK5+YqAiS42PrsFLGGFO3wgr0IjJSRFaJyFoRubWK8e1F5DMRWSIis0QkNWRcOxGZLiIrRGS592jB2lGpD32617UyIsL60Btj6q8DBnoRiQSeBEYBPYFLRaRnpWIPAS+rah9gPHB/yLiXgQdVtQcwCNheExWvUtY6QKBZR8C7WMrSNsaYei6cFv0gYK2qrlfVImAScH6lMj2Bz73XM0vHez8IUao6A0BVc1U1r0ZqXpWstdA0FaJdcE/flW83MzPG1HvhBPo2QFrI+3T2fdj3YmCs93oM0FhEkoBuQLaIvC0iC0XkQe8IoQIRGSci80RkXmZm5sGvRamQHjcFxQF25BZai94YU+/V1MnYm4FhIrIQGAZkAAEgCjjVGz8Q6ARcXXliVZ2gqgNUdUBKSsqh1UDVtei9E7GbS+9aaS16Y0w9F06gzwDahrxP9YaVUdXNqjpWVfsBt3vDsnGt/0Ve2qcEmAr0r4F67ytvp+teWalrpbXojTH1XTiBfi7QVUQ6ikgMcAnwXmgBEUkWkdJ53QZMDJk2QURKm+lnAMsPv9pViIqFsc9B158A5fehT21mtz8wxtRvBwz0Xkv8euATYAUwWVWXich4ERntFRsOrBKR1UAL4D5v2gAubfOZiPwACPBcja8FQGw89LkYkrsCrsdNZITQorH1oTfG1G9R4RRS1WnAtErD7gx5PQWYUs20M4A+h1HHQ5KRnU/LJg2IivTXNWHGGHOwfBsF03fl2YlYY4zBx4E+w/rQG2MM4NNAHwwqW/cU0LqpBXpjjPFloC8sCRJUaBQb1ikIY4zxNZ8G+gAADaJ9uXrGGHNQfBkJC4qDAMRG7XO3BWOMqXd8GehLW/SxUb5cPWOMOSi+jISFJV6L3lI3xhjj00BvqRtjjCnjz0BvqRtjjCnjy0hYmrppEG0temOM8WWgLyi2Fr0xxpTyZSS0k7HGGFPOl5GwPEdvqRtjjPFnoC/rdePL1TPGmIPiy0hoJ2ONMaacTwO9nYw1xphSYUVCERkpIqtEZK2I3FrF+PYi8pmILBGRWSKSGjIuICKLvL/3Kk9bGwosdWOMMWUOeB9fEYkEngTOBtKBuSLynqqGPuT7IeBlVX1JRM4A7geu9Mblq2rfmq32/hWWBIiMEHuMoDHGEF6LfhCwVlXXq2oRMAk4v1KZnsDn3uuZVYw/ogqLg9aaN8YYTzjRsA2QFvI+3RsWajEw1ns9BmgsIkne+wYiMk9E5ojIBVUtQETGeWXmZWZmhl/7ahSWWKA3xphSNRUNbwaGichCYBiQAQS8ce1VdQBwGfCIiHSuPLGqTlDVAao6ICUl5bArU1gSsB43xhjjCedZexlA25D3qd6wMqq6Ga9FLyLxwIWqmu2Ny/D+rxeRWUA/YN3hVnx/rEVvjDHlwomGc4GuItJRRGKAS4AKvWdEJFlESud1GzDRG54oIrGlZYChQOhJ3FpRUBywq2KNMcZzwECvqiXA9cAnwApgsqouE5HxIjLaKzYcWCUiq4EWwH3e8B7APBFZjDtJ+0Cl3jq1orAkaPe5McYYTzipG1R1GjCt0rA7Q15PAaZUMd03QO/DrONBs143xhhTzpfRsLDEUjfGGFPKp4E+SANL3RhjDODTQG8nY40xppwvA711rzTGmHK+jIbW68YYY8r5MhoWWurGGGPK+DPQW+rGGGPK+C4aqqqXurEWvTHGgA8DfeljBK1Fb4wxju+ioQV6Y4ypyHfRsOx5sZa6McYYwI+B3p4Xa4wxFfguGlrqxhhjKvJdNCxN3dgTpowxxvFdoC+w1I0xxlTgu2hYdjLWrow1xhjAl4Hea9HbvW6MMQYIM9CLyEgRWSUia0Xk1irGtxeRz0RkiYjMEpHUSuObiEi6iDxRUxWvjvW6McaYig4YDUUkEngSGAX0BC4VkZ6Vij0EvKyqfYDxwP2Vxt8LfHH41T0wS90YY0xF4TR7BwFrVXW9qhYBk4DzK5XpCXzuvZ4ZOl5ETsQ9MHz64Vf3wEpTN/aEKWOMccKJhm2AtJD36d6wUIuBsd7rMUBjEUkSkQjg38DN+1uAiIwTkXkiMi8zMzO8mlejsNha9MYYE6qmmr03A8NEZCEwDMgAAsBvgWmqmr6/iVV1gqoOUNUBKSkph1UROxlrjDEVRYVRJgNoG/I+1RtWRlU347XoRSQeuFBVs0VkCHCqiPwWiAdiRCRXVfc5oVtT7MpYY4ypKJxAPxfoKiIdcQH+EuCy0AIikgzsVNUgcBswEUBVLw8pczUwoDaDPJSnbmIiLdAbYwyEkbpR1RLgeuATYAUwWVWXich4ERntFRsOrBKR1bgTr/fVUn0PqPTpUiJSV1UwxpijSjgtelR1GjCt0rA7Q15PAaYcYB4vAi8edA0PUmFJ0O5zY4wxIXyX3ygoDlh+3hhjQvguIrrnxfputYwx5pD5LiIWlgSsD70xxoTwX6AvDlrqxhhjQvguItrJWGOMqciHgd5OxhpjTCjfRcQCS90YY0wFvouIdjLWGGMq8mGgt+6VxhgTyncR0XrdGGNMRb6LiIUlAet1Y4wxIXwY6K1Fb4wxoXwVEVXVu9eNteiNMaaUrwJ9SVAJqj10xBhjQvkqItpjBI0xZl++ioj2YHBjjNmXvwK916JvYC16Y4wpE1ZEFJGRIrJKRNaKyD7PfBWR9iLymYgsEZFZIpIaMnyBiCwSkWUi8n81vQKhyh8Mbi16Y4wpdcBALyKRwJPAKKAncKmI9KxU7CHgZVXtA4wH7veGbwGGqGpf4CTgVhFpXUN130dBWerGWvTGGFMqnGfGDgLWqup6ABGZBJwPLA8p0xO4yXs9E5gKoKpFIWViqeVUkZ2MNab+Ki4uJj09nYKCgrquSq1q0KABqampREdHhz1NOIG+DZAW8j4d1zoPtRgYCzwKjAEai0iSqmaJSFvgQ6ALcIuqbq68ABEZB4wDaNeuXdiVr8xOxhpTf6Wnp9O4cWM6dOiAiNR1dWqFqpKVlUV6ejodO3YMe7qaavreDAwTkYXAMCADCHgVS/NSOl2Aq0SkReWJVXWCqg5Q1QEpKSmHXInyHL216I2pbwoKCkhKSvJtkAcQEZKSkg76qCWciJgBtA15n+oNK6Oqm1V1rKr2A273hmVXLgMsBU49qBoehPJeN9aiN6Y+8nOQL3Uo6xhOoJ8LdBWRjiISA1wCvFdpwckiUjqv24CJ3vBUEWnovU4ETgFWHXQtw1RYYidjjTF1Izs7m6eeeuqgpzvnnHPIzs6u+QqFOGBEVNUS4HrgE2AFMFlVl4nIeBEZ7RUbDqwSkdVAC+A+b3gP4DsRWQzMBh5S1R9qeB3KFBRb90pjTN2oLtCXlJTsd7pp06aRkJBQS7VywjkZi6pOA6ZVGnZnyOspwJQqppsB9DnMOoatrEVvvW6MMUfYrbfeyrp16+jbty/R0dE0aNCAxMREVq5cyerVq7ngggtIS0ujoKCAG264gXHjxgHQoUMH5s2bR25uLqNGjeKUU07hm2++oU2bNrz77rs0bNjwsOsWVqA/VhQW28lYYwzc8/4ylm/eU6Pz7Nm6CXed16va8Q888ABLly5l0aJFzJo1i3PPPZelS5eW9Y6ZOHEizZo1Iz8/n4EDB3LhhReSlJRUYR5r1qzh9ddf57nnnuPiiy/mrbfe4oorrjjsuvsr0NuVscaYo8SgQYMqdIF87LHHeOeddwBIS0tjzZo1+wT6jh070rdvXwBOPPFENmzYUCN18Vmgt5Oxxhj22/I+Uho1alT2etasWXz66ad8++23xMXFMXz48Cq7SMbGxpa9joyMJD8/v0bq4quIWFgSJCYygogI/3exMsYcXRo3bkxOTk6V43bv3k1iYiJxcXGsXLmSOXPmHNG6+apF754u5avfLmPMMSIpKYmhQ4dy/PHH07BhQ1q0KL82dOTIkTzzzDP06NGD7t27M3jw4CNaN18F+sKSoPW4McbUmddee63K4bGxsXz00UdVjivNwycnJ7N06dKy4TfffHON1ctXUbGwOGgnYo0xphJ/BfoSS90YY0xlvoqKLnVjLXpjjAnlv0BvLXpjjKnAV1HRet0YY8y+fBUVLXVjjDH78legtxa9MaaOHOptigEeeeQR8vLyarhG5XwVFYssR2+MqSNHc6D33QVT9nQpY0xdCL1N8dlnn03z5s2ZPHkyhYWFjBkzhnvuuYe9e/dy8cUXk56eTiAQ4G9/+xvbtm1j8+bNnH766SQnJzNz5swar5vPAr2lbowxwEe3wtYafsZRy94w6oFqR4fepnj69OlMmTKF77//HlVl9OjRfPHFF2RmZtK6dWs+/PBDwN0Dp2nTpjz88MPMnDmT5OTkmq2zx1dRscCujDXGHAWmT5/O9OnT6devH/3792flypWsWbOG3r17M2PGDP7yl7/w5Zdf0rRp0yNSn7Ba9CIyEngUiAT+q6oPVBrfHvec2BRgJ3CFqqaLSF/gaaAJEADuU9U3aq76FRWWBOxeN8aY/ba8jwRV5bbbbuPXv/71PuMWLFjAtGnTuOOOOzjzzDO58847q5hDzTpgVBSRSOBJYBTQE7hURHpWKvYQ8LKq9gHGA/d7w/OAX6hqL2Ak8IiIJNRQ3SsIBJXigFrqxhhTJ0JvUzxixAgmTpxIbm4uABkZGWzfvp3NmzcTFxfHFVdcwS233MKCBQv2mbY2hNOiHwSsVdX1ACIyCTgfWB5Spidwk/d6JjAVQFVXlxZQ1c0ish3X6s8+3IpXVuQ9XcpOxhpj6kLobYpHjRrFZZddxpAhQwCIj4/n1VdfZe3atdxyyy1EREQQHR3N008/DcC4ceMYOXIkrVu3rrOTsW2AtJD36cBJlcosBsbi0jtjgMYikqSqWaUFRGQQEAOsq7wAERkHjANo167dwdS/jD1dyhhT1yrfpviGG26o8L5z586MGDFin+l+//vf8/vf/77W6lVTUfFmYJiILASGARm4nDwAItIKeAW4RlWDlSdW1QmqOkBVB6SkpBxSBUSEc/u0olNK/CFNb4wxfhVOiz4DaBvyPtUbVkZVN+Na9IhIPHChqmZ775sAHwK3q2qtPT+racNonrysf23N3hhjjlnhtOjnAl1FpKOIxACXAO+FFhCRZBEpnddtuB44eOXfwZ2onVJz1TbGGBOuAwZ6VS0Brgc+AVYAk1V1mYiMF5HRXrHhwCoRWQ20AO7zhl8MnAZcLSKLvL++NbwOxhgDuG6Nfnco6yhH24YZMGCAzps3r66rYYw5xvz44480btyYpKQkRKSuq1MrVJWsrCxycnLo2LFjhXEiMl9VB1Q1na9ugWCMqb9SU1NJT08nMzOzrqtSqxo0aEBqaupBTWOB3hjjC9HR0fu0co1jnc6NMcbnLNAbY4zPWaA3xhifO+p63YhIJrDxMGaRDOyooeocK+rjOkP9XO/6uM5QP9f7YNe5vapWeWuBoy7QHy4RmVddFyO/qo/rDPVzvevjOkP9XO+aXGdL3RhjjM9ZoDfGGJ/zY6CfUNcVqAP1cZ2hfq53fVxnqJ/rXWPr7LscvTHGmIr82KI3xhgTwjeBXkRGisgqEVkrIrfWdX1qi4i0FZGZIrJcRJaJyA3e8GYiMkNE1nj/E+u6rjVNRCJFZKGIfOC97ygi33n7/A3vtti+IiIJIjJFRFaKyAoRGeL3fS0if/Q+20tF5HURaeDHfS0iE0Vku4gsDRlW5b4V5zFv/ZeIyEE9fMMXgT7MB5j7RQnwJ1XtCQwGfuet663AZ6raFfjMe+83N+BulV3qn8B/VLULsAu4tk5qVbseBT5W1eOAE3Dr79t9LSJtgD8AA1T1eCAS9wwMP+7rF4GRlYZVt29HAV29v3HA0wezIF8EekIeYK6qRUDpA8x9R1W3qOoC73UO7ovfBre+L3nFXgIuqJMK1hIRSQXOBf7rvRfgDKD0gTZ+XOemuOc5PA+gqkXek9t8va9xN1tsKCJRQBywBR/ua1X9AthZaXB1+/Z83AOc1HtSX4L3iNaw+CXQV/UA8zZ1VJcjRkQ6AP2A74AWqrrFG7UV9wAYP3kE+DNQ+szhJCDbezAO+HOfdwQygRe8lNV/RaQRPt7XqpoBPARswgX43cB8/L+vS1W3bw8rxvkl0Nc73rN53wJuVNU9oePUdaXyTXcqEfkpsF1V59d1XY6wKKA/8LSq9gP2UilN48N9nYhrvXYEWgON2De9US/U5L71S6A/4APM/UREonFB/n+q+rY3eFvpoZz3f3td1a8WDAVGi8gGXFruDFzuOsE7vAd/7vN0IF1Vv/PeT8EFfj/v67OAH1U1U1WLgbdx+9/v+7pUdfv2sGKcXwL9AR9g7hdebvp5YIWqPhwy6j3gKu/1VcC7R7putUVVb1PVVFXtgNu3n6vq5cBM4GdeMV+tM4CqbgXSRKS7N+hMYDk+3te4lM1gEYnzPuul6+zrfR2iun37HvALr/fNYGB3SIrnwFTVF3/AOcBqYB1we13XpxbX8xTc4dwSYJH3dw4uZ/0ZsAb4FGhW13WtpfUfDnzgve4EfA+sBd4EYuu6frWwvn2Bed7+ngok+n1fA/cAK4GlwCtArB/3NfA67jxEMe7o7drq9i0guJ6F64AfcL2Swl6WXRlrjDE+55fUjTHGmGpYoDfGGJ+zQG+MMT5ngd4YY3zOAr0xxvicBXpjjPE5C/TGGONzFuiNMcbn/h/F6lQVN11zIgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 1)\n",
    "fig.suptitle('Accuracy for test and training sets')\n",
    "plt.plot(model_history['accuracy'], label = 'train')\n",
    "plt.plot(model_history['val_accuracy'], label = 'test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12749f567798517b8543354a13719bbd42e9e3e56a89ba27a040f4f72d5c2230"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
